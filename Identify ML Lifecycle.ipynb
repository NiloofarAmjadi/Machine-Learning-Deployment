{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f2cb9bd",
   "metadata": {},
   "source": [
    "In this discussion, you must describe various ways (at least two) to identify the features that are most significant in predicting or classifying the target feature. Provide references and give python code examples (MLO2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb317a8",
   "metadata": {},
   "source": [
    "Feature selection is a critical step in building effective machine learning models. Identifying the features that are most significant in predicting or classifying the target feature can enhance model performance, reduce overfitting, and improve interpretability. There are several methods to identify significant features, including statistical methods, model-based methods, and techniques from machine learning libraries.\n",
    "\n",
    "\n",
    "1. Statistical Feature Selection\n",
    "Statistical methods evaluate the relationship between each feature and the target variable. These methods are useful for initial feature selection and understanding feature importance. One common statistical method is the use of correlation coefficients for numerical data or chi-square tests for categorical data.\n",
    "\n",
    "Correlation Coefficient\n",
    "The correlation coefficient measures the linear relationship between two variables. Features with a high correlation (either positive or negative) with the target variable are considered significant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b0c213b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target      1.000000\n",
      "Feature1    0.021895\n",
      "Feature2   -0.061987\n",
      "Feature3   -0.115117\n",
      "Name: Target, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Generate sample data\n",
    "np.random.seed(0)\n",
    "data = pd.DataFrame({\n",
    "    'Feature1': np.random.rand(100),\n",
    "    'Feature2': np.random.rand(100),\n",
    "    'Feature3': np.random.rand(100),\n",
    "    'Target': np.random.rand(100)\n",
    "})\n",
    "\n",
    "# Compute correlation matrix\n",
    "correlation_matrix = data.corr()\n",
    "\n",
    "# Get correlation of each feature with the target\n",
    "target_corr = correlation_matrix['Target'].sort_values(ascending=False)\n",
    "print(target_corr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85492aa8",
   "metadata": {},
   "source": [
    "Chi-Square Test\n",
    "The chi-square test assesses the independence of two categorical variables. For feature selection, it tests whether the feature and target are independent. A low p-value indicates a significant relationship.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ad054fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Feature  Chi2 Score       p-value\n",
      "2  petal length (cm)  116.312613  5.533972e-26\n",
      "3   petal width (cm)   67.048360  2.758250e-15\n",
      "0  sepal length (cm)   10.817821  4.476515e-03\n",
      "1   sepal width (cm)    3.710728  1.563960e-01\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load sample data\n",
    "data = load_iris()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target)\n",
    "\n",
    "# Apply chi-square test\n",
    "chi2_scores, p_values = chi2(X, y)\n",
    "\n",
    "# Create a DataFrame with the results\n",
    "chi2_results = pd.DataFrame({'Feature': X.columns, 'Chi2 Score': chi2_scores, 'p-value': p_values})\n",
    "print(chi2_results.sort_values('Chi2 Score', ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d80ec9",
   "metadata": {},
   "source": [
    "2. Model-Based Feature Selection\n",
    "Model-based methods use machine learning algorithms to determine feature importance. These methods fit a model to the data and derive importance scores from the model itself. Common model-based methods include decision trees and regularized regression techniques like Lasso.\n",
    "Decision Trees\n",
    "Decision trees inherently perform feature selection by choosing features that best split the data at each node. The importance of each feature can be derived from the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52c08a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Feature  Importance\n",
      "3   petal width (cm)    0.922611\n",
      "2  petal length (cm)    0.064056\n",
      "0  sepal length (cm)    0.013333\n",
      "1   sepal width (cm)    0.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "\n",
    "# Load sample data\n",
    "data = load_iris()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target)\n",
    "\n",
    "# Fit decision tree model\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "importances = pd.DataFrame({'Feature': X.columns, 'Importance': model.feature_importances_})\n",
    "print(importances.sort_values('Importance', ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001c62a4",
   "metadata": {},
   "source": [
    "Lasso Regression\n",
    "Lasso (Least Absolute Shrinkage and Selection Operator) regression is a type of linear regression that uses L1 regularization to penalize the absolute size of the coefficients. This can shrink some coefficients to zero, effectively performing feature selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5088cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Feature  Coefficient\n",
      "0      MedInc     0.390583\n",
      "1    HouseAge     0.015082\n",
      "4  Population     0.000018\n",
      "2    AveRooms    -0.000000\n",
      "3   AveBedrms     0.000000\n",
      "5    AveOccup    -0.003323\n",
      "7   Longitude    -0.099225\n",
      "6    Latitude    -0.114214\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "\n",
    "# Load sample data\n",
    "housing = fetch_california_housing()\n",
    "X = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "y = pd.Series(housing.target)\n",
    "\n",
    "# Fit Lasso model\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "lasso_importances = pd.DataFrame({'Feature': X.columns, 'Coefficient': lasso.coef_})\n",
    "print(lasso_importances.sort_values('Coefficient', ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e223c3",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "Feature selection is a crucial step in the machine learning pipeline. Statistical methods such as correlation coefficients and chi-square tests provide an initial assessment of feature significance. Model-based methods, including decision trees and Lasso regression, offer a more sophisticated approach by leveraging the power of machine learning algorithms to identify important features. Each method has its strengths and is suitable for different types of data and problem contexts. Combining these approaches often yields the best results, providing a robust feature selection strategy.\n",
    "\n",
    "\n",
    "\n",
    "References\n",
    "1.\tTowards Data Science: https://towardsdatascience.com/feature-selection-techniques-in-machine-learning-with-python-f24e7da3f36e  \n",
    "2.\tHastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer.\n",
    "3.\tPedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., ... & Vanderplas, J. (2011). Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research, 12, 2825-2830.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
